{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods\n",
    "\n",
    "repeatedly drawing samples from a training set and refitting a model of interest on each sample to *obtain additional information about the fitted model*\n",
    "\n",
    "- e.g. estimate the variability of a linear regression fit\n",
    "\n",
    "- in general: **Test Error Performance**\n",
    "\n",
    "What happens in the absense of a large designated test set?\n",
    "\n",
    " * mathematical adjustment to the training error rate in order to estimate the test error rate\n",
    " \n",
    " * CV, consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting   process, and than applying the statistical learning method to those held-out observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Validation Set Approach\n",
    "\n",
    "**Procedure:** \n",
    " 1. randomly divide data into two parts, a training set anf a validation set/ hold-out set\n",
    " \n",
    " 2. fit the model on the training set\n",
    " \n",
    " 3. use fitted model to predict observations for the validation set\n",
    " \n",
    " 4. validation error rate provides an estimate of the test error rate\n",
    " \n",
    "But: test MSE depends on the specific hold-out set!\n",
    "\n",
    " - if we repeat this we will get a high variance in the test error rate.\n",
    " \n",
    " - less observation in the training sample let perform statistical models worse $\\Rightarrow$ overestimate test error rate\n",
    " \n",
    "### What is if we dont split validation set and training set equally?\n",
    "\n",
    "Nothing, there is allways a Trade-off between **bias** and **variance**. \n",
    "\n",
    " * increasing size of training set $\\Rightarrow$ less bias\n",
    " \n",
    " * increasing size of validation set $\\Rightarrow$ less variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out CV (LOOCV)\n",
    "\n",
    "**Procedure:**\n",
    " * instead of creating two subsets, just leave one observation out $(x_1,y_1)$ and $\\{(x_2,y_2),\\dots,(x_n,y_n)\\}$\n",
    " \n",
    " * fit model on $n-1$ observations\n",
    " \n",
    " * $MSE_1 = (y_1-\\hat{y}_1)^2$\n",
    " \n",
    " * repeat this $n$ times\n",
    " \n",
    " $$CV_{(n)}=\\frac{1}{n} \\sum_{i=1}^n MSE_i$$\n",
    "\n",
    "**Advantages:**\n",
    " - less bias - more observations for training set, tends not to overestimate the test error rate\n",
    " \n",
    " - less variance - always yields same results, no randomness in training set splits\n",
    " \n",
    "**Disadvantages:**\n",
    " - computational expensive, since we have to calculate CV $n$ times\n",
    "\n",
    "But: there exist a short-cut which inflates residuals for high leverage points (see *James et. al. 2013*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation (CV)\n",
    "\n",
    "**Procedure:**\n",
    " 1. randomly divide data into $k$ groups (approx. equal size)\n",
    " \n",
    " 2. first group is validation set, while $k-1$ are training set\n",
    " \n",
    " 3. fit model on remaining $k-1$ folds\n",
    " \n",
    " 4. compute MSE on valdiation set\n",
    " \n",
    " 5. repeat procedure $k$ times\n",
    " \n",
    "$$CV_{(k)}=\\frac{1}{K} \\sum_{i=1}^K MSE_i$$\n",
    "\n",
    "Rule-of-Thumb: $k=5$ or $k=10$ \n",
    "\n",
    "Note: if $k=n$ then n-fold CV is equal to LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-off\n",
    "\n",
    " - Validation set approach overestimates the test error rate $\\rightarrow$ bias!\n",
    " \n",
    " - LOOCV is unbiased but has higher variance\n",
    " \n",
    " - k-Fold CV intermediate level of both\n",
    " \n",
    "**Bias reduction:** \n",
    "LOOCV > k-Fold CV\n",
    "\n",
    "**Variance reduction:**\n",
    "k-Fold CV > LOOCV    (k<n)\n",
    "\n",
    "**Why is this the case?**\n",
    " * LOOCV averages the outputs of $n$ fitted models which are highly correlated (almost identical sets)\n",
    " \n",
    " * k-Fold CV (k<n) averages outputs of k fitted models which are less correlated\n",
    " \n",
    "## CV on Classification\n",
    "can be applied to classification problems as well:\n",
    "$$CV_{(n)}=\\frac{1}{n} \\sum_{i=1}^n I(y_i \\neq \\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bootstrap\n",
    "\n",
    "To assess the variability of the coefficient estimtes & predictions form a statistical learning method.\n",
    "This is done by **repeatedly sampling** from the same dataset **with replacement**. ($B$ times resampling with the same sample size)\n",
    "\n",
    "**Procedure:**\n",
    " 1. generate Bootstrap sample $Z_1^*,\\dots,Z_n^* \\quad \\text{i.i.d. } \\sim \\hat{F}_n$ (e.g. $n$ uniform resamplings with replacement)\n",
    " \n",
    " 2. compute Bootstrap estimate (e.g. mean, OLS, ML, etc.)     $\\hat{\\theta}_n^*=g(Z_1^*,\\dots,Z_n^*)$\n",
    " \n",
    " 3. Repeat steps until $\\hat{\\theta}_n^{*1}, \\dots, \\hat{\\theta}_n^{*B}$\n",
    " \n",
    " 4. Bootstraped estimators are:\n",
    " $$ E(\\hat{\\theta}^*) \\approx \\frac{1}{B} \\sum_{i=1}^B \\hat{\\theta}_n^{*i} \\\\\n",
    "    Var(\\hat{\\theta}^*) \\approx \\frac{1}{B-1} \\sum_{i=1}^B \\left(\\hat{\\theta}_n^{*i}-\\frac{1}{B} \\sum_{j=1}^B \\hat{\\theta}_n^{*j}\\right)^2 $$\n",
    "    \n",
    "**Summary:**\n",
    "\n",
    "Bootstrapping allows **estimation of the sampling distribution** of almost any statistic using random sampling methods.\n",
    "\n",
    "Hence, it allows assigning measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates.\n",
    "\n",
    " - Bootstrapping the distribution of $\\hat{y}$ $\\Rightarrow$ confidence interval (CI)\n",
    " \n",
    " - Bootstrapping the distribution of $\\hat{\\beta}$ $\\Rightarrow$ variance/ standard error (SE)\n",
    " \n",
    "Since we are able to estimate the sample distribution via Bootstrap, we do not need to make assumptions on a theoretical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises on Resampling Methods\n",
    "Consider the following data generating process in which n observations belong to one of two classes. There are two covariates, drawn from normal distribution $x_1\\sim N$ and $x_2 \\sim N$ with class specific means. The class means are $\\mu_1 = (âˆ’3 \\quad 3)$ for class $1$, and $\\mu_2 = (5 \\quad 5)$ for class $2$ and $\\Sigma_1 = \\Sigma_2$. Initially, you may set $\\Sigma=(16 \\quad -2)(-2 \\quad 9)$ and $n_1 =300$ and $n_2 =500$.\n",
    "Use the general procedure for generating the dataframe and for calculating the lda and the qda function from exercise 1 in the last problem set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear workspace\n",
    "rm(list=ls())\n",
    "# Load packages that will be needed\n",
    "library(MASS)\n",
    "#Set-up:\n",
    "n1 = 300\n",
    "mu1= c(-3,3)\n",
    "n2 = 500\n",
    "mu2= c(5,5)\n",
    "covmat=matrix(c(16,-2,-2,9), nrow=2, ncol=2)\n",
    "N=n1+n2\n",
    "\n",
    "#Creating training data\n",
    "set.seed(100)\n",
    "X1=mvrnorm(n = n1, mu = mu1, Sigma = covmat)\n",
    "X2=mvrnorm(n = n2, mu = mu2, Sigma = covmat)\n",
    "df1 = data.frame(cbind(X1,rep(1,length(X1[,1]))))\n",
    "df2 = data.frame(cbind(X2,rep(2,length(X2[,1]))))\n",
    "df = merge(df1,df2, all=TRUE)\n",
    "colnames(df)=c(\"X1\",\"X2\",\"class\")\n",
    "rm(df1, df2, X1, X2)\n",
    "\n",
    "#Creating test data\n",
    "set.seed(200)\n",
    "X1_test=mvrnorm(n = n1, mu = mu1, Sigma = covmat)\n",
    "X2_test=mvrnorm(n = n2, mu = mu2, Sigma = covmat)\n",
    "df1_test = data.frame(cbind(X1_test,rep(1,length(X1_test[,1]))))\n",
    "df2_test = data.frame(cbind(X2_test,rep(2,length(X2_test[,1]))))\n",
    "df_test = merge(df1_test,df2_test, all=TRUE)\n",
    "colnames(df_test)=c(\"X1\",\"X2\",\"class\")\n",
    "rm(df1_test, df2_test, X1_test, X2_test)\n",
    "\n",
    "# LDA analysis\n",
    "LDA = lda(class ~ X1 + X2, data=df)\n",
    "class_lfit = as.numeric(predict(LDA)$class)\n",
    "class_lfit_test = as.numeric(predict(LDA,df_test)$class)\n",
    "\n",
    "# QDA analysis\n",
    "QDA = qda(class ~ X1 + X2 , data=df)\n",
    "class_qfit = as.numeric(predict(QDA)$class)\n",
    "class_qfit_test = as.numeric(predict(QDA,df_test)$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculate the mean training error and the mean test error using a new data set with the same specifications as the training data for both methods and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          \n",
       "class_lfit   1   2\n",
       "         1 241  51\n",
       "         2  59 449"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "          \n",
       "class_qfit   1   2\n",
       "         1 240  51\n",
       "         2  60 449"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Mean Error LDA:  0.1375'</span>"
      ],
      "text/latex": [
       "'Mean Error LDA:  0.1375'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Mean Error LDA:  0.1375'</span>"
      ],
      "text/plain": [
       "[1] \"Mean Error LDA:  0.1375\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Mean Error QDA:  0.13875'</span>"
      ],
      "text/latex": [
       "'Mean Error QDA:  0.13875'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Mean Error QDA:  0.13875'</span>"
      ],
      "text/plain": [
       "[1] \"Mean Error QDA:  0.13875\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "               \n",
       "class_lfit_test   1   2\n",
       "              1 245  51\n",
       "              2  55 449"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "               \n",
       "class_qfit_test   1   2\n",
       "              1 245  53\n",
       "              2  55 447"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Mean Test Error LDA:  0.1325'</span>"
      ],
      "text/latex": [
       "'Mean Test Error LDA:  0.1325'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Mean Test Error LDA:  0.1325'</span>"
      ],
      "text/plain": [
       "[1] \"Mean Test Error LDA:  0.1325\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Mean Test Error QDA:  0.135'</span>"
      ],
      "text/latex": [
       "'Mean Test Error QDA:  0.135'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Mean Test Error QDA:  0.135'</span>"
      ],
      "text/plain": [
       "[1] \"Mean Test Error QDA:  0.135\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Comparison LDA and QDA for training data\n",
    "table(class_lfit,df[,3])\n",
    "table(class_qfit,df[,3])\n",
    "mean_error_lda=(1/N)*sum(class_lfit!=df[,3])\n",
    "mean_error_qda=(1/N)*sum(class_qfit!=df[,3])\n",
    "paste('Mean Error LDA: ', mean_error_lda)\n",
    "paste('Mean Error QDA: ', mean_error_qda)\n",
    "\n",
    "#Comparison LDA and QDA for test data\n",
    "table(class_lfit_test,df_test[,3])\n",
    "table(class_qfit_test,df_test[,3])\n",
    "mean_error_lda_test=(1/N)*sum(class_lfit_test!=df_test[,3])\n",
    "mean_error_qda_test=(1/N)*sum(class_qfit_test!=df_test[,3])\n",
    "paste('Mean Test Error LDA: ', mean_error_lda_test)\n",
    "paste('Mean Test Error QDA: ', mean_error_qda_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Write a function that uses the validation set approach for generating test and training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls())\n",
    "library(MASS)\n",
    "\n",
    "n1=500\n",
    "n2=500\n",
    "N=n1+n2\n",
    "mu1=c(-7,8)\n",
    "mu2=c(3,8)\n",
    "covmat1=matrix(c(16,-2,-2,9), nrow=2, ncol=2)\n",
    "covmat2=matrix(c(16,-2,-2,9), nrow=2, ncol=2)\n",
    "c=0.5\n",
    "\n",
    "#create function that uses defined variables to create data set and\n",
    "#randomly splits it in samples with fraction of training data equal to c\n",
    "validation_set = function(n1=n1,n2=n2,mu1=mu1,mu2=mu2,covmat1=covmat1,\n",
    "                          covmat2=covmat2,c=c){\n",
    "  #create data set\n",
    "  X1=mvrnorm(n = n1, mu = mu1, Sigma = covmat1)\n",
    "  X2=mvrnorm(n = n2, mu = mu2, Sigma = covmat2)\n",
    "  df1 = data.frame(cbind(X1,rep(1,length(X1[,1]))))\n",
    "  df2 = data.frame(cbind(X2,rep(2,length(X2[,1]))))\n",
    "  df = merge(df1,df2, all=TRUE)\n",
    "  colnames(df)=c(\"X1\",\"X2\",\"class\")\n",
    "\n",
    "  #randomly split data in fraction c and 1-c\n",
    "  sample = sample.int(n=nrow(df), size=floor(c*nrow(df)), replace=F)\n",
    "  training_sample = df[sample,]\n",
    "  test_sample = df[-sample,]\n",
    "\n",
    "  return(list(training_sample=training_sample,test_sample=test_sample))\n",
    "}\n",
    "\n",
    "validation_samples=validation_set(n1,n2,mu1,mu2,covmat1,covmat2,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that performs k-fold cross-validation for generating test and training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables that should be used to create sample\n",
    "n1=500\n",
    "n2=500\n",
    "N=n1+n2\n",
    "mu1=c(-7,8)\n",
    "mu2=c(3,8)\n",
    "covmat1=matrix(c(16,-2,-2,9), nrow=2, ncol=2)\n",
    "covmat2=matrix(c(16,-2,-2,9), nrow=2, ncol=2)\n",
    "k=5  #number of folds\n",
    "\n",
    "k_fold = function(n1=n1,n2=n2,mu1=mu1,mu2=mu2,covmat1=covmat1,\n",
    "                  covmat2=covmat2,k=k){\n",
    "  #create data set\n",
    "  X1=mvrnorm(n = n1, mu = mu1, Sigma = covmat1)\n",
    "  X2=mvrnorm(n = n2, mu = mu2, Sigma = covmat2)\n",
    "  df1 = data.frame(cbind(X1,rep(1,length(X1[,1]))))\n",
    "  df2 = data.frame(cbind(X2,rep(2,length(X2[,1]))))\n",
    "  df = merge(df1,df2,all=TRUE)\n",
    "  colnames(df)=c(\"X1\",\"X2\",\"class\")\n",
    "  rm(X1,X2,df1,df2)\n",
    "\n",
    "  #randomly split data in k folds\n",
    "  fold_list=list()\n",
    "  naming=c(1:k)\n",
    "  for (i in 1:k){\n",
    "    fold = sample.int(n=nrow(df), size=(N/k), replace=F)\n",
    "    fold_list[[i]]= data.frame(df[fold,])\n",
    "    names(fold_list)[[i]]=naming[i]\n",
    "    df=df[-fold,]}\n",
    "\n",
    "  return(fold_list)\n",
    "}\n",
    "\n",
    "folds=k_fold(n1,n2,mu1,mu2,covmat1,covmat2,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation Study**\n",
    "Evaluate the difference between the lda and qda methods through calculating classification training and test error for 100 different simulation runs. For each run, compare the difference between the\n",
    "\n",
    "a) validation set approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'training data:'"
      ],
      "text/latex": [
       "'training data:'"
      ],
      "text/markdown": [
       "'training data:'"
      ],
      "text/plain": [
       "[1] \"training data:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'LDA: 0.05259'"
      ],
      "text/latex": [
       "'LDA: 0.05259'"
      ],
      "text/markdown": [
       "'LDA: 0.05259'"
      ],
      "text/plain": [
       "[1] \"LDA: 0.05259\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'QDA: 0.05258'"
      ],
      "text/latex": [
       "'QDA: 0.05258'"
      ],
      "text/markdown": [
       "'QDA: 0.05258'"
      ],
      "text/plain": [
       "[1] \"QDA: 0.05258\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'test data:'"
      ],
      "text/latex": [
       "'test data:'"
      ],
      "text/markdown": [
       "'test data:'"
      ],
      "text/plain": [
       "[1] \"test data:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'LDA: 0.05259'"
      ],
      "text/latex": [
       "'LDA: 0.05259'"
      ],
      "text/markdown": [
       "'LDA: 0.05259'"
      ],
      "text/plain": [
       "[1] \"LDA: 0.05259\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'QDA: 0.05298'"
      ],
      "text/latex": [
       "'QDA: 0.05298'"
      ],
      "text/markdown": [
       "'QDA: 0.05298'"
      ],
      "text/plain": [
       "[1] \"QDA: 0.05298\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=100\n",
    "c=0.5\n",
    "\n",
    "Mean_error=matrix(NaN,n,2)\n",
    "Mean_error_test=matrix(NaN,n,2)\n",
    "for(i in 1:n){\n",
    "  df=validation_set(n1,n2,mu1,mu2,covmat1,covmat2,c)$training_sample\n",
    "  df_test=validation_set(n1,n2,mu1,mu2,covmat1,covmat2,c)$test_sample\n",
    "\n",
    "  LDA = lda(class ~ X1 + X2, data=df)\n",
    "  class_lfit = as.numeric(predict(LDA)$class)\n",
    "  class_lfit_test = as.numeric(predict(LDA,df_test)$class)\n",
    "\n",
    "  QDA = qda(class ~ X1 + X2 , data=df)\n",
    "  class_qfit = as.numeric(predict(QDA)$class)\n",
    "  class_qfit_test = as.numeric(predict(QDA,df_test)$class)\n",
    "\n",
    "  Mean_error[i,1] = (1/N)*sum(class_lfit!=df[,3])\n",
    "  Mean_error[i,2] = (1/N)*sum(class_qfit!=df[,3])\n",
    "\n",
    "  Mean_error_test[i,1] = (1/N)*sum(class_lfit_test!=df_test[,3])\n",
    "  Mean_error_test[i,2] = (1/N)*sum(class_qfit_test!=df_test[,3])\n",
    "}\n",
    "\n",
    "#difference in average error of LDA compared to QDA\n",
    "paste('training data:')\n",
    "paste('LDA:',avg_lda=mean(Mean_error[,1]))\n",
    "paste('QDA:',avg_qda=mean(Mean_error[,2]))\n",
    "paste('test data:')\n",
    "paste('LDA:',avg_lda_test=mean(Mean_error_test[,1]))\n",
    "paste('QDA:',avg_qda_test=mean(Mean_error_test[,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Comparing LDA and QDA of evaluation samples: 0.000175000000000008'"
      ],
      "text/latex": [
       "'Comparing LDA and QDA of evaluation samples: 0.000175000000000008'"
      ],
      "text/markdown": [
       "'Comparing LDA and QDA of evaluation samples: 0.000175000000000008'"
      ],
      "text/plain": [
       "[1] \"Comparing LDA and QDA of evaluation samples: 0.000175000000000008\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Comparing LDA and QDA of test samples: -0.000600000000000003'"
      ],
      "text/latex": [
       "'Comparing LDA and QDA of test samples: -0.000600000000000003'"
      ],
      "text/markdown": [
       "'Comparing LDA and QDA of test samples: -0.000600000000000003'"
      ],
      "text/plain": [
       "[1] \"Comparing LDA and QDA of test samples: -0.000600000000000003\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Comparing average mean error of LDA for evaluation and test samples - difference btw. test and train: -0.00434999999999999'"
      ],
      "text/latex": [
       "'Comparing average mean error of LDA for evaluation and test samples - difference btw. test and train: -0.00434999999999999'"
      ],
      "text/markdown": [
       "'Comparing average mean error of LDA for evaluation and test samples - difference btw. test and train: -0.00434999999999999'"
      ],
      "text/plain": [
       "[1] \"Comparing average mean error of LDA for evaluation and test samples - difference btw. test and train: -0.00434999999999999\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'Comparing average mean error of QDA for evaluation and test samples - difference btw. test and train: -0.005125'"
      ],
      "text/latex": [
       "'Comparing average mean error of QDA for evaluation and test samples - difference btw. test and train: -0.005125'"
      ],
      "text/markdown": [
       "'Comparing average mean error of QDA for evaluation and test samples - difference btw. test and train: -0.005125'"
      ],
      "text/plain": [
       "[1] \"Comparing average mean error of QDA for evaluation and test samples - difference btw. test and train: -0.005125\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=5\n",
    "\n",
    "Mean_error_lda=matrix(NaN,n,k)\n",
    "Mean_error_qda=matrix(NaN,n,k)\n",
    "Mean_error_test_lda=matrix(NaN,n,k)\n",
    "Mean_error_test_qda=matrix(NaN,n,k)\n",
    "for(i in 1:n){\n",
    "  for (l in 1:k){\n",
    "    total_sample=k_fold(n1,n2,mu1,mu2,covmat1,covmat2,k)\n",
    "    pre_sample=total_sample[-l]\n",
    "    sample=do.call(rbind, pre_sample)\n",
    "    names(sample)=c(\"X1\",\"X2\",\"class\")\n",
    "    test_sample=data.frame(total_sample[l])\n",
    "    names(test_sample)=c(\"X1\",\"X2\",\"class\")\n",
    "\n",
    "    LDA = lda(class ~ X1 + X2, data=sample)\n",
    "    class_lfit = as.numeric(predict(LDA)$class)\n",
    "    class_lfit_test = as.numeric(predict(LDA,test_sample)$class)\n",
    "\n",
    "    QDA = qda(class ~ X1 + X2 , data=sample)\n",
    "    class_qfit = as.numeric(predict(QDA)$class)\n",
    "    class_qfit_test = as.numeric(predict(QDA,test_sample)$class)\n",
    "\n",
    "    num=nrow(sample)\n",
    "    num1=nrow(test_sample)\n",
    "\n",
    "    Mean_error_lda[i,l] = (1/num)*sum(class_lfit!=sample$class)\n",
    "    Mean_error_qda[i,l] = (1/num)*sum(class_qfit!=sample$class)\n",
    "\n",
    "    Mean_error_test_lda[i,l] = (1/num1)*sum(class_lfit_test!=test_sample$class)\n",
    "    Mean_error_test_qda[i,l] = (1/num1)*sum(class_qfit_test!=test_sample$class)\n",
    "  }}\n",
    "\n",
    "\n",
    "#Creating a matrix to compare average mean error of LDA and QDA\n",
    "#First column average mean error of the 5 folds for n runs of LDA\n",
    "#Second column average mean error of the 5 folds for n runs of QDA\n",
    "Mean_error=cbind(rowMeans(Mean_error_lda),rowMeans(Mean_error_qda))\n",
    "\n",
    "\n",
    "#Same thing for the test samples\n",
    "Mean_error_test=cbind(rowMeans(Mean_error_test_lda),rowMeans(Mean_error_test_qda))\n",
    "\n",
    "rslt1<-mean(Mean_error_lda[,1])-mean(Mean_error_qda[,1])\n",
    "paste('Comparing LDA and QDA of evaluation samples:', rslt1)\n",
    "\n",
    "rslt2<-mean(Mean_error_test_lda[,1])-mean(Mean_error_test_qda[,1])\n",
    "paste('Comparing LDA and QDA of test samples:', rslt2)\n",
    "\n",
    "rslt3<-mean(Mean_error_lda[,1])-mean(Mean_error_test_lda[,1])\n",
    "paste('Comparing average mean error of LDA for evaluation and test samples - difference btw. test and train:', rslt3)\n",
    "\n",
    "rslt4<-mean(Mean_error_qda[,1])-mean(Mean_error_test_qda[,1])\n",
    "paste('Comparing average mean error of QDA for evaluation and test samples - difference btw. test and train:', rslt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"enter k=10 in line 167 and run b) again\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the theoretical properties discussed in the lecture, propose a suitable metric for comparing these three\n",
    "methods and compare the results of your simulation study with your expectations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
